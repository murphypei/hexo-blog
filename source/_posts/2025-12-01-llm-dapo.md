---
title: 大模型训练方法：DAPO
date: 2025-12-01 16:20:00
update: 2025-12-01 16:20:00
categories: LLM
tags: [LLM, DAPO, GRPO, 强化学习, 策略优化]
mathjax: true
---

DAPO (Decoupled Clip and Dynamic Sampling Policy Optimization) 作为大模型强化学习训练的新兴算法，通过四项核心改进有效解决了GRPO在长序列优化中的痛点问题，在数学推理等复杂任务中取得了显著的性能提升。

<!-- more -->

## DAPO 简介

在大模型强化学习（RL）训练领域，OpenAI 的 O1、DeepSeek 的 R1 等模型凭借出色的复杂任务表现，证明了大规模 RL 训练的巨大价值。但这些顶尖模型的核心训练技术细节长期未公开，而主流的 GRPO 算法在长链式思维（CoT）等复杂场景中，又频繁面临熵崩溃、训练不稳定等问题。

**DAPO（Decoupled Clip and Dynamic Sampling Policy Optimization）** 算法针对这些问题，提出了四项关键改进，在保持训练稳定性的同时显著提升了模型在复杂推理任务上的表现。本文将深入剖析 DAPO 的技术原理，重点解析其核心改进点及实际应用价值。

## DAPO 的技术背景与核心定位

### 传统 RL 训练的痛点
GRPO 作为大模型 RL 训练的主流算法，虽能提升模型性能，但在数学推理等长 CoT 场景中暴露出诸多缺陷：

1. **熵崩溃**：固定剪裁范围限制了低概率 token 的探索，导致策略快速收敛，生成内容同质化严重
2. **梯度失效**：训练后期易出现大量全优或全差样本，优势函数趋近于零，梯度信号消失，浪费训练资源  
3. **梯度稀释**：长序列样本的梯度经样本级平均后被弱化，关键推理步骤的优化信号无法有效传递
4. **奖励噪声**：对超长响应的刚性截断惩罚，易误判有效推理内容，干扰模型对奖励信号的学习

### DAPO 的核心目标

DAPO 以"解决长序列 RL 优化难题、提供开源可复现方案、突破复杂任务性能上限"为核心目标。其算法设计围绕"释放模型探索能力、提升训练信号质量、精准优化长序列"三大方向展开，在 AIME 等数学推理任务中取得了显著的性能提升。

## DAPO 的四大核心改进点深度解析

DAPO 通过解耦剪辑策略、动态采样机制、token 级损失计算和长度感知奖励修正四项革新，针对性解决了 GRPO 的核心痛点。以下结合技术原理、解决路径和实际效果展开说明：

### 改进一：Clip-Higher——解耦高低剪辑范围，平衡探索与利用

#### 传统算法的局限
GRPO 和 PPO 均采用固定的对称剪辑范围（如 $\epsilon=0.2$），将新策略与旧策略的概率比值限制在 $[1-\epsilon, 1+\epsilon]$ 区间。传统的剪辑函数为：

$$clip(r_t, 1-\epsilon, 1+\epsilon) = \max(\min(r_t, 1+\epsilon), 1-\epsilon)$$

其中 $r_t = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ 是重要性采样比率。

这种设计虽能避免策略突变，但会严重抑制低概率关键 token 的优化。当模型偶然生成一个对推理至关重要但概率极低的 token 时，其概率比值极易超出剪辑上限而被截断，导致该 token 无法获得有效强化，最终模型探索能力衰退，陷入熵崩溃。

#### DAPO 的解决方案

DAPO 提出**解耦式剪辑策略**，将上下剪辑阈值拆分为独立参数：

$$clip_{higher}(r_t, \epsilon_{low}, \epsilon_{high}) = \max(\min(r_t, 1+\epsilon_{high}), 1-\epsilon_{low})$$

其中：
- 低阈值 $\epsilon_{low}=0.2$：保持与传统算法一致，抑制高概率 token 的过度利用，避免模型陷入局部最优
- 高阈值 $\epsilon_{high}=0.28$：放宽对高比值的限制，为低概率关键 token 提供足够的上涨空间

这种非对称设计既保证了训练稳定性，又释放了模型的探索潜力。例如在数学推理中，模型生成的"辅助变量假设"等低概率关键推理步骤，能通过该机制获得有效强化，逐步形成更完整的推理链条。

#### 实际效果

该改进使模型生成多样性显著提升，训练过程中熵值保持稳定。通过允许更大的上行剪辑范围，模型能够更好地学习低概率但高价值的 token，有效缓解了熵崩溃问题。

### 改进二：Dynamic Sampling——动态过滤无效样本，强化梯度信号

#### 传统采样的核心问题

训练过程中，当模型性能提升到一定阶段，易出现大量全正确或全错误的样本。这些样本的优势函数值为零，对应的梯度信号也会消失，导致"采样量大但有效信息少"的资源浪费。

在强化学习中，优势函数定义为：
$$A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$$

当奖励为极端值（0 或 1）时，$A(s_t, a_t) \approx 0$，导致策略梯度：
$$\nabla_\theta J(\theta) = \mathbb{E}[A(s_t, a_t) \nabla_\theta \log \pi_\theta(a_t|s_t)]$$

趋近于零，训练效率严重下降。

#### DAPO 的动态采样策略

DAPO 引入**梯度有效性筛选机制**，具体算法如下：

```python
def dynamic_sampling(model, prompts, target_batch_size):
    effective_samples = []
    sampling_attempts = 0
    
    while len(effective_samples) < target_batch_size:
        # 生成候选样本
        candidates = sample_responses(model, prompts)
        
        # 筛选有效样本（奖励不为极端值）
        for sample in candidates:
            if 0 < sample.reward < 1:  # 排除极端奖励
                effective_samples.append(sample)
                
        sampling_attempts += 1
        if sampling_attempts > max_attempts:
            break
    
    return effective_samples[:target_batch_size]
```

该机制的核心逻辑是：若当前采样结果中无有效梯度样本，则触发二次采样，直至批次中充满具备梯度价值的样本。由于过滤操作仅需校验奖励值，计算开销极低，不会显著增加训练时间。

#### 实际效果

动态采样使模型收敛速度大幅提升，通过确保每个训练批次都包含有效的梯度信号，避免了训练后期因梯度失效导致的模型性能震荡。实验表明，该机制能够显著提升训练效率。

### 改进三：Token-Level Policy Gradient Loss——精准优化长序列，避免梯度稀释

#### 长序列优化的核心痛点

GRPO 采用"样本内平均+批次内平均"的双层损失计算方式。传统的损失函数为：

$$L = \frac{1}{B} \sum_{i=1}^{B} \left[ \frac{1}{T_i} \sum_{t=1}^{T_i} \ell_t^{(i)} \right]$$

其中 $B$ 是批次大小，$T_i$ 是第 $i$ 个样本的序列长度，$\ell_t^{(i)}$ 是第 $i$ 个样本第 $t$ 个 token 的损失。

这种方式对长序列极不友好：一个 200 token 的长回答与一个 10 token 的短回答，在样本级平均后，每个 token 的梯度权重分别为 $1/200$ 和 $1/10$。长回答中的关键推理 token（如公式推导、逻辑转折）的梯度被严重稀释，模型难以捕捉长链式推理的核心规律。

#### DAPO 的 token 级损失计算

DAPO 将损失聚合方式从**样本级**改为**全局 token 级**：

$$L_{token} = \frac{1}{\sum_{i=1}^{B} T_i} \sum_{i=1}^{B} \sum_{t=1}^{T_i} w_t^{(i)} \cdot \ell_t^{(i)}$$

其中 $w_t^{(i)}$ 是第 $i$ 个样本第 $t$ 个 token 的权重。

具体优化包括：

1. **取消样本内梯度平均**：直接计算每个 token 的独立损失
2. **全局归一化**：按批次内所有 token 的总数进行归一化，而非按样本数量归一化  
3. **重要性权重**：对长序列中的关键 token（如数学公式、逻辑连词）可额外增加权重

```python
def token_level_loss(batch_outputs, advantages):
    total_tokens = sum(len(output) for output in batch_outputs)
    token_losses = []
    
    for output, advantage in zip(batch_outputs, advantages):
        for token_id, token_advantage in zip(output, advantage):
            # 每个token独立计算损失，无样本内平均
            token_loss = -log_prob(token_id) * token_advantage
            token_losses.append(token_loss)
    
    # 全局token级归一化
    return sum(token_losses) / total_tokens
```

这确保长、短回答的每个 token 拥有公平的梯度权重，长序列中的有效信息不再被稀释。

#### 实际效果

该改进让模型的长序列生成能力显著增强。在需要多步骤推理的数学题求解中，模型能稳定生成结构完整的推理过程，重复、乱码等低质量模式大幅减少，训练稳定性也得到明显提升。

### 改进四：Overlong Reward Shaping——软惩罚机制，降低奖励噪声

#### 传统长度惩罚的缺陷

传统算法对超长响应采用刚性惩罚（如超过长度阈值直接扣 1 分），这种方式存在两大问题：

1. **误判有效内容**：将有效长推理与无效冗余内容同等惩罚，例如完整的数学推导因超长被误判为低质量
2. **引入奖励噪声**：突然的惩罚会引入剧烈奖励噪声，干扰模型对有效推理模式的学习

#### DAPO 的长度感知奖励修正

DAPO 设计了**分段式软惩罚函数**，根据响应长度动态调整惩罚力度：

$$r_{\text{length}}(y) = \begin{cases}
0, & |y| \leq l_{\text{max}} - l_{\text{cache}} \\
\frac{(l_{\text{max}} - l_{\text{cache}}) - |y|}{l_{\text{cache}}}, & l_{\text{max}} - l_{\text{cache}} < |y| \leq l_{\text{max}} \\
-1, & |y| > l_{\text{max}}
\end{cases}$$

其中 $l_{\text{max}}$ 为最大长度阈值，$l_{\text{cache}}$ 为缓冲长度。该函数实现三层惩罚逻辑：

1. **正常长度区间**：无惩罚，鼓励完整推理
2. **缓冲区间**：惩罚随长度线性增加，避免轻微超长被过度惩罚  
3. **超长区间**：刚性惩罚，过滤无意义的冗余内容

最终的奖励函数为：
$$r_{\text{final}}(y) = r_{\text{original}}(y) + \alpha \cdot r_{\text{length}}(y)$$

其中 $\alpha$ 是长度惩罚的权重系数。

同时，DAPO 会过滤截断样本的损失计算，进一步减少噪声对训练的干扰：

```python
def compute_length_penalty(response_length, max_length=2048, cache_length=512):
    if response_length <= max_length - cache_length:
        return 0.0
    elif response_length <= max_length:
        return (max_length - cache_length - response_length) / cache_length
    else:
        return -1.0
```

#### 实际效果

软惩罚机制使训练波动大幅降低，模型既能生成足够长度的推理内容，又能有效抑制无意义的冗余输出，实现了推理完整性与简洁性的平衡。

## DAPO 的整体训练流程与性能表现

### 完整训练流程

DAPO 的训练流程整合了四项核心改进，具体步骤如下：

1. **数据预处理**：输入 prompt 与高奖励回复数据，标注 token 级别重要性与响应长度信息
2. **动态采样**：过滤无效样本，构建梯度有效的训练批次
3. **策略优化**：基于 Clip-Higher 计算策略比值，通过 token 级损失函数反向传播梯度
4. **奖励修正**：采用软惩罚函数调整超长响应的奖励值，更新模型参数
5. **迭代优化**：循环上述步骤，直至模型熵值稳定且任务准确率达到目标

### 算法伪代码

```python
def dapo_training(model, dataset, epochs):
    for epoch in range(epochs):
        # 1. 动态采样
        effective_batch = dynamic_sampling(model, dataset.prompts)
        
        # 2. 计算重要性采样比率 (Clip-Higher)
        ratios = []
        for sample in effective_batch:
            old_prob = old_model.log_prob(sample)
            new_prob = model.log_prob(sample)
            ratio = exp(new_prob - old_prob)
            clipped_ratio = clip_higher(ratio, eps_low=0.2, eps_high=0.28)
            ratios.append(clipped_ratio)
        
        # 3. Token级损失计算
        loss = compute_token_level_loss(effective_batch, ratios)
        
        # 4. 长度感知奖励修正
        adjusted_rewards = []
        for sample in effective_batch:
            length_penalty = compute_length_penalty(len(sample))
            adjusted_reward = sample.reward + length_penalty
            adjusted_rewards.append(adjusted_reward)
        
        # 5. 反向传播更新
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

### 性能表现分析

实验结果显示 DAPO 在数学推理任务上的显著提升：

| 技术组件 | 基线性能 | 性能提升 | 累计提升 |
|----------|----------|----------|----------|
| 基础 GRPO | 30% | - | - |
| + 动态采样 | 34% | +4% | +4% |
| + Clip-Higher | 36% | +2% | +6% |
| + Token级损失 | 39% | +3% | +9% |
| + 软惩罚机制 | 42% | +3% | +12% |
| 完整 DAPO 系统 | 45% | +3% | +15% |

从数据可见，DAPO 的四项改进形成了协同效应，每项改进都对最终性能有积极贡献。特别值得注意的是，训练过程中模型还自发演化出"反思修正"等高级推理能力，证明其优化方向贴合复杂任务的核心需求。

## 总结与展望

### 核心贡献

DAPO 的核心价值体现在以下几个方面：

1. **系统性解决方案**：针对 GRPO 在长序列训练中的四大痛点，提出了对应的解决方案
2. **技术创新**：解耦剪辑策略和 token 级损失计算等创新设计，为强化学习优化提供了新思路
3. **实用性**：显著的性能提升证明了算法的实际价值，特别是在数学推理等复杂任务上

### 技术优势总结

| 改进点 | 解决问题 | 技术手段 | 核心优势 |
|--------|----------|----------|----------|
| Clip-Higher | 熵崩溃 | 非对称剪辑范围 | 平衡探索与稳定性 |
| Dynamic Sampling | 梯度失效 | 有效样本筛选 | 提升训练效率 |
| Token-Level Loss | 梯度稀释 | 全局token归一化 | 公平优化长序列 |
| Soft Length Penalty | 奖励噪声 | 分段式软惩罚 | 平滑奖励信号 |

### 未来发展方向

DAPO 的优化方向可能集中在以下几个方面：

1. **自适应参数调整**：根据训练阶段和任务特性，动态调整 Clip-Higher 的阈值参数
2. **智能权重分配**：结合注意力机制，为不同类型的 token 分配更精准的权重
3. **多任务适配**：扩展到更多领域的复杂推理任务，验证算法的通用性
4. **计算优化**：进一步优化动态采样的计算效率，降低训练开销

### 实际应用价值

对于研究人员而言，DAPO 提供了一套完整的长序列强化学习优化框架，为探索更高效的大模型训练路径提供了重要参考。对于工业界来说，其高效的训练策略为降低大规模模型的训练成本、提升复杂任务性能提供了实用的解决方案。

通过解决长序列强化学习的核心技术难题，DAPO 为大模型在复杂推理任务上的应用奠定了更加坚实的基础。
