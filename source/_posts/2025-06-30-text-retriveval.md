---
title: LLM：RAG 中的文本检索技术
date: 2025-06-30 03:19:02
update: 2025-06-30 03:19:02
categories: LLM
tags: [LLM, RAG, 文本检索, 向量检索, BM25, RAG]
mathjax: true
---
继续准备 LLM 面试知识，这次写文本检索技术。文本检索是 RAG（检索增强生成）系统的核心组件，也是面试中经常被问到的问题。本文将详细介绍稠密向量检索、稀疏向量检索、BM25算法以及混合检索策略，帮助理解现代文本检索系统的技术原理。

<!-- more -->

## 引言

在当今的信息检索领域，随着人工智能和自然语言处理技术的发展，文本检索技术已经从传统的基于关键词匹配的方法，发展到了基于深度学习的语义检索方法。文本检索是RAG（Retrieval-Augmented Generation）系统的核心组件，它负责从大规模文档集合中检索出与用户查询最相关的文档片段，为后续的生成模型提供上下文信息。

本文将详细介绍三种主要的文本检索方法：**稠密向量检索（Dense Retrieval）**、**稀疏向量检索（Sparse Retrieval）**和**BM25算法**，以及它们的混合使用策略。

## 稠密向量检索（Dense Retrieval）

### 稠密向量检索的基本原理

稠密向量检索是一种基于深度学习的检索方法，它通过将文本转换为高维空间中的连续向量表示，然后使用向量相似度（如余弦相似度、欧氏距离）来检索相关文档。

**核心思想**：
- 将查询和文档都映射到同一个高维向量空间
- 通过计算向量间的相似度来衡量相关性
- 能够捕捉文本的深层语义信息

### 技术实现

#### 1. 文本编码

稠密向量检索通常使用预训练的语言模型（如BERT、T5、Sentence-BERT等）对文本进行编码：

$$
\mathbf{q} = \text{Encoder}(query) \\
\mathbf{d} = \text{Encoder}(document)
$$

其中：
- $\mathbf{q}$ 是查询的向量表示
- $\mathbf{d}$ 是文档的向量表示
- $\text{Encoder}$ 是预训练的语言模型

#### 2. 相似度计算

常用的相似度计算方法包括：

**余弦相似度**：
$$
\text{sim}(\mathbf{q}, \mathbf{d}) = \frac{\mathbf{q} \cdot \mathbf{d}}{|\mathbf{q}| \cdot |\mathbf{d}|}
$$

**点积相似度**：
$$
\text{sim}(\mathbf{q}, \mathbf{d}) = \mathbf{q} \cdot \mathbf{d}
$$

**欧氏距离**：
$$
\text{dist}(\mathbf{q}, \mathbf{d}) = \sqrt{\sum_{i=1}^{n} (q_i - d_i)^2}
$$

#### 3. 索引和检索

**向量索引**：
- 使用FAISS、Annoy、HNSW等向量索引库
- 支持高效的近似最近邻搜索
- 能够处理百万级别的向量检索

**检索流程**：
1. 将查询编码为向量
2. 在向量索引中搜索最相似的文档向量
3. 返回相似度最高的文档

### 稠密向量检索的优势和局限性

**优势**：
1. **语义理解能力强**：能够理解查询和文档的深层语义
2. **处理同义词和近义词**：即使词汇不完全匹配，也能找到相关文档
3. **支持复杂查询**：能够处理自然语言形式的查询

**局限性**：
1. **计算成本高**：需要深度学习模型进行编码
2. **索引规模限制**：在大规模数据集上可能遇到性能瓶颈
3. **对训练数据敏感**：检索效果依赖于编码模型的训练质量

### 应用场景

- **智能问答系统**：从知识库中检索相关答案
- **推荐系统**：基于内容相似性推荐相关文档
- **语义搜索**：理解用户意图的搜索引擎

## 稀疏向量检索（Sparse Retrieval）

### 稀疏向量检索的基本原理

稀疏向量检索是基于传统信息检索模型的方法，它使用词袋模型（Bag of Words）将文本表示为稀疏向量，并通过计算词频-逆文档频率（TF-IDF）来评估文档与查询的相关性。

**核心思想**：
- 将文本表示为高维稀疏向量
- 每个维度对应词汇表中的一个词
- 通过统计方法计算词的重要性

### 技术实现

#### 1. TF-IDF计算

**词频（Term Frequency, TF）**：
$$
\text{TF}(t, d) = \frac{\text{词 } t \text{ 在文档 } d \text{ 中的出现次数}}{\text{文档 } d \text{ 的总词数}}
$$

**逆文档频率（Inverse Document Frequency, IDF）**：
$$
\text{IDF}(t) = \log \frac{\text{总文档数}}{\text{包含词 } t \text{ 的文档数}}
$$

**TF-IDF权重**：
$$
\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \text{IDF}(t)
$$

#### 2. 向量构建

文档向量 $\mathbf{d}$ 的构建：
$$
\mathbf{d} = [\text{TF-IDF}(t_1, d), \text{TF-IDF}(t_2, d), ..., \text{TF-IDF}(t_n, d)]
$$

其中 $t_1, t_2, ..., t_n$ 是词汇表中的所有词。

#### 3. 相似度计算

**余弦相似度**：
$$
\text{sim}(\mathbf{q}, \mathbf{d}) = \frac{\mathbf{q} \cdot \mathbf{d}}{|\mathbf{q}| \cdot |\mathbf{d}|}
$$

**点积相似度**：
$$
\text{sim}(\mathbf{q}, \mathbf{d}) = \mathbf{q} \cdot \mathbf{d}
$$

### 稀疏向量检索的优势和局限性

**优势**：
1. **计算效率高**：基于统计方法，计算速度快
2. **可解释性强**：能够明确知道哪些词贡献了相关性
3. **处理大规模数据**：能够高效处理大规模文档集合

**局限性**：
1. **语义理解能力弱**：无法处理同义词和近义词
2. **词汇匹配限制**：需要查询词在文档中实际出现
3. **无法处理语义相似性**：无法理解词汇的深层含义

### 应用场景

- **传统搜索引擎**：基于关键词的网页搜索
- **文档检索系统**：从文档库中检索相关文档
- **信息过滤**：基于关键词的信息过滤

## BM25算法

### BM25算法的基本原理

BM25（Best Matching 25）是一种经典的信息检索算法，它是TF-IDF算法的改进版，通过引入词频（TF）和文档频率（DF）的函数来计算文档与查询的相关性得分。

**核心思想**：
- 在TF-IDF基础上引入文档长度归一化
- 使用词频饱和函数处理高频词
- 通过参数调整优化检索效果

### 数学公式

BM25算法的核心公式：

$$
\text{BM25}(Q, D) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{\text{avgdl}})}
$$

其中：
- $Q$ 是查询，包含词 $q_1, q_2, ..., q_n$
- $D$ 是文档
- $f(q_i, D)$ 是词 $q_i$ 在文档 $D$ 中的词频
- $|D|$ 是文档 $D$ 的长度
- $\text{avgdl}$ 是文档集合的平均长度
- $k_1$ 和 $b$ 是调节参数

**IDF计算**：
$$
\text{IDF}(q_i) = \log \frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}
$$

其中：
- $N$ 是文档集合的总文档数
- $n(q_i)$ 是包含词 $q_i$ 的文档数

### 参数调节

**$k_1$ 参数**：
- 控制词频饱和程度
- 通常设置为 1.2-2.0
- 值越大，词频的影响越线性

**$b$ 参数**：
- 控制文档长度归一化程度
- 取值范围为 0-1
- $b=0$ 表示不进行长度归一化
- $b=1$ 表示完全归一化

### BM25算法的优势

1. **理论基础扎实**：基于概率检索模型
2. **参数可调节**：能够适应不同的数据集和需求
3. **计算效率高**：基于统计方法，计算速度快
4. **效果稳定**：在许多基准测试中表现优异

### 应用场景

- **搜索引擎**：Google、Bing等搜索引擎的核心算法
- **文档检索**：企业文档管理系统
- **学术搜索**：学术论文检索系统

## 混合检索策略

### 混合检索的基本原理

混合检索结合了稠密向量检索、稀疏向量检索和BM25算法的优势，通过多路召回和结果融合来提高检索系统的整体性能。

**核心思想**：
- 使用多种检索方法并行检索
- 通过融合算法合并检索结果
- 平衡准确性和召回率

### 混合检索的优势

1. **互补性**：不同方法各有优势，相互补充
2. **提高准确性**：通过多路召回提高检索准确性
3. **提升召回率**：增加检索结果的覆盖面
4. **适应性**：能够适应不同的查询类型和场景

### 实现方法

#### 1. 多路召回

**稠密向量检索**：
- 使用语义相似性进行检索
- 适合处理语义相关的查询

**稀疏向量检索**：
- 使用关键词匹配进行检索
- 适合处理精确匹配的查询

**BM25检索**：
- 使用传统信息检索方法
- 适合处理结构化查询

#### 2. 结果融合

**RRF（Reciprocal Rank Fusion）**：
$$
\text{RRF}(d) = \sum_{i=1}^{n} \frac{1}{k + \text{rank}_i(d)}
$$

其中：
- $\text{rank}_i(d)$ 是文档 $d$ 在第 $i$ 个检索方法中的排名
- $k$ 是调节参数，通常设置为 60

**加权融合**：
$$
\text{Score}(d) = \sum_{i=1}^{n} w_i \cdot \text{score}_i(d)
$$

其中：
- $w_i$ 是第 $i$ 个检索方法的权重
- $\text{score}_i(d)$ 是文档 $d$ 在第 $i$ 个检索方法中的得分

#### 3. 动态权重调整

根据查询类型动态调整不同检索方法的权重：

- **语义查询**：增加稠密向量检索的权重
- **关键词查询**：增加稀疏向量检索和BM25的权重
- **混合查询**：平衡各种方法的权重

### 系统架构

**查询处理层**：
- 查询解析和预处理
- 查询类型识别
- 参数选择

**检索层**：
- 多路并行检索
- 结果初步排序
- 去重和合并

**融合层**：
- 结果融合算法
- 最终排序
- 结果返回

## 性能评估指标

### 检索性能指标

**准确率（Precision）**：
$$
\text{Precision} = \frac{\text{相关文档数}}{\text{检索文档数}}
$$

**召回率（Recall）**：
$$
\text{Recall} = \frac{\text{相关文档数}}{\text{总相关文档数}}
$$

**F1分数**：
$$
\text{F1} = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
$$

**平均精度（MAP）**：
$$
\text{MAP} = \frac{1}{|Q|} \sum_{q \in Q} \text{AP}(q)
$$

其中 $\text{AP}(q)$ 是查询 $q$ 的平均精度。

### 效率指标

**检索延迟**：从查询到返回结果的时间
**吞吐量**：单位时间内处理的查询数
**索引大小**：索引占用的存储空间

## 实际应用中的优化策略

### 索引优化

**倒排索引**：
- 为每个词建立文档列表
- 支持快速的关键词查找
- 优化存储和查询效率

**向量索引**：
- 使用HNSW、IVF等算法
- 支持高效的近似最近邻搜索
- 平衡精度和速度

### 查询优化

**查询扩展**：
- 使用同义词扩展查询
- 基于用户反馈调整查询
- 利用查询日志优化

**查询重写**：
- 将自然语言查询转换为结构化查询
- 使用查询模板提高效率
- 基于历史查询进行优化

### 缓存策略

**结果缓存**：
- 缓存热门查询的结果
- 使用LRU等策略管理缓存
- 提高响应速度

**索引缓存**：
- 将常用索引加载到内存
- 使用分层缓存策略
- 优化内存使用
