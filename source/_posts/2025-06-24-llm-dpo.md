---
title: LLM 训练：DPO 深入与实践
date: 2025-06-24 20:44:51
update: 2026-01-12 15:10:00
categories: LLM
tags: [LLM, RLHF, DPO]
mathjax: true
---

本文聚焦于 Direct Preference Optimization（DPO）在 LLM 对齐训练中的原理与实践。文章先从直观动机与数学推导入手，给出训练流程与实现要点，随后比较 DPO 与基于奖励的 PPO 在适用场景、训练复杂度与稳定性上的异同，最后给出工程建议与常见陷阱。

<!-- more -->

## 1. 为什么选择 DPO？

DPO 的核心动机是用人类偏好对模型直接建模，避免单独训练一个奖励模型带来的偏差与不稳定性。与传统的 RLHF（先训练 Reward Model，再用 RL 算法优化策略）相比，DPO 把偏好学习直接挪到策略优化目标中，流程更简单、计算更高效。

适用场景：

- 只有偏好数据（pairwise preferences），没有或不想训练单独奖励模型
- 希望减少训练复杂度、提高迭代速度的工程化场景
- 需要明确保留参考策略能力（SFT 行为）同时提升偏好表现

但 DPO 也有局限：对偏好数据质量敏感、探索能力弱、对温度参数敏感。

## 2. 基本设定与直观公式

给定一个提示 $x$，以及两个回答 $y_w$（被人类标注为更好）与 $y_l$（被标注为较差），DPO 以偏好对 $(x,y_w,y_l)$ 为训练样本，直接学习策略 $\pi_\theta$，并使用一个固定的参考策略 $\pi_{\mathrm{ref}}$（通常为 SFT 模型）来提供基线约束。

核心思想可以用以下三步理解：

1. 以 log-prob 差表示相对偏好
2. 用 sigmoid/log-sigmoid 构造对数似然损失
3. 通过最大化偏好对的对数似然来优化策略

形式化地，定义隐式奖励为：
$$
r_\theta(x,y) = \beta \log \frac{\pi_\theta(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}
$$
这里 $\beta>0$ 为温度因子，控制敏感度。

将 Bradley–Terry 型偏好模型代入，DPO 的训练损失为：

$$
L_{\mathrm{DPO}}(\theta) = -\mathbb{E}_{(x,y_w,y_l)\sim D} \left[\log \sigma\left(r_\theta(x,y_w) - r_\theta(x,y_l)\right)\right],
$$
其中 $\sigma(z)=1/(1+e^{-z})$。

直观上，若策略对 $y_w$ 的相对提升明显（$r_\theta(x,y_w)\gg r_\theta(x,y_l)$），则损失接近 0；若相反则损失很大，驱动参数更新。

## 3. 数学推导要点与数值实现

1) 从偏好概率出发：Bradley–Terry 假设

$$
P(y_w\succ y_l\mid x) = \frac{\exp(r(x,y_w))}{\exp(r(x,y_w)) + \exp(r(x,y_l))} = \sigma\big(r(x,y_w)-r(x,y_l)\big).
$$

2) 将奖励替换成策略对数比：$r(x,y)=\beta\log\frac{\pi_\theta(y|x)}{\pi_{\mathrm{ref}}(y|x)}$，得到上面的 DPO 损失。

3) 数值稳定与实现细节：

- 计算时使用对数概率之差，避免直接 exponent/ratio 运算。对于自回归模型，序列对数概率为 token log-prob 的累加：
  $$\log\pi_\theta(y\mid x)=\sum_{t=1}^T \log\pi_\theta(y_t\mid x, y_{<t}).$$
- 在实现上直接计算 $\Delta = \beta\left(\log\pi_\theta(y_w|x)-\log\pi_\theta(y_l|x) - (\log\pi_{\mathrm{ref}}(y_w|x)-\log\pi_{\mathrm{ref}}(y_l|x))\right)$，再用 `log-sigmoid(Δ)`。
- 为数值稳定建议使用带有 `log-sum-exp` 或框架自带的 `logsigmoid`/`softplus` 函数，避免溢出/下溢。

4) 梯度与参照策略：

DPO 只更新 $\pi_\theta$ 的参数，$\pi_{\mathrm{ref}}$ 保持冻结。梯度项直接来自 $\partial_\theta \log\sigma(\Delta)$，可以通过自动微分在模型上直接计算。

## 4. 训练流程（工程化建议）

一个典型的 DPO 训练步骤：

1. 数据准备：收集或构造偏好对 $(x,y_w,y_l)$，做必要的去重与清洗。
2. 模型初始化：用 SFT 模型初始化 $\pi_\theta$，并将其拷贝为 $\pi_{\mathrm{ref}}$（并冻结 $\pi_{\mathrm{ref}}$）。
3. 批次构造：每个 batch 包含若干偏好对，计算两条序列在策略和参考模型下的对数概率。
4. 损失计算：对每个偏好对计算 $\Delta$，聚合负对数 sigmoid 损失。
5. 反向传播与优化：仅更新 $\pi_\theta$，可使用 Adam/AdamW。
6. 周期性评估：在验证偏好集上评估准确率、NLL 等指标；检查生成质量（一致性、流畅性、有害内容）。

工程要点：

- 批量化计算两条序列的 log-prob 可以共享 prefix 的前向计算以节省算力
- 使用 mixed-precision 时注意 log-prob 的数值精度
- 对长序列可截断或分段计算 log-prob，然后累加
- 参考模型占用显存，若显存紧张可在 CPU 上或按小批次推理参考模型

## 5. 与 PPO 的对比（要点速览）

下面给出一张简表，突出两者在原理与工程上的核心差异：

| 维度 | DPO | PPO (基于 RM 的 RLHF) |
|---:|---|---|
| 所需模型 | 策略 + 参考 | Actor + Critic + Reward + Reference |
| 数据 | 偏好对 (pairwise) | 可为评分或偏好；通常需训练 Reward Model |
| 优势估计 | 无需 GAE/TD（直接用差） | 需要 GAE/TD、密集价值估计 |
| 训练复杂度 | 低 | 高（采样、回放、价值训练） |
| 采样探索 | 相对弱 | 可通过环境采样增强探索 |
| 稳定性 | 对数据质量敏感；训练稳定且简单 | 裁剪机制提升稳定性；但训练管线复杂 |
| 适用场景 | 只有偏好数据或想降低复杂度 | 可训练 RM 或需要更强探索时 |

实务建议：若你有大量高质量偏好对且想快速迭代，优先尝试 DPO；若需要在任务中强化探索或有复杂回报结构，PPO 依然更灵活。

## 6. 超参数与调优建议

- 温度 $\beta$：通常 0.05–0.5。$\beta$ 越大，策略差异导致的奖励放大越明显，但也更容易过拟合偏好噪声。
- 学习率：同 SFT 微调类似起点（1e-5—5e-5），根据模型规模和批次大小调整。
- 批次大小：尽量保证每个 batch 包含多样的偏好对，避免过拟合单一 prompt。
- 参考策略频率：通常保持固定，但也可周期性用当前策略更新参考（慎用，会改变训练目标）。

调优技巧：先在小规模数据上做超参搜索，关注偏好准确率和生成样本的可读性指标；使用 early stopping 防止过拟合。

## 7. 常见问题与陷阱

1. 偏好数据偏差：标签者偏好差异会直接反映到模型行为上。需要多标注/去噪/纠偏。
2. 参考模型选择：若 $\pi_{\mathrm{ref}}$ 能力过弱或过强，都会影响训练效果；常用 SFT 作为平衡选择。
3. 伪造优化（gaming the metric）：模型可能学会优化 log-prob 差的“捷径”而不是语义质量。需要人工抽样检查。
4. 温度过大：可能引起梯度爆炸或过拟合罕见偏好。

## 8. 实战示例（伪代码）

以下为训练一个 batch 的核心步骤（伪代码）：

```
for (x,y_w,y_l) in batch:
    lp_w_theta = logprob(theta, x, y_w)
    lp_l_theta = logprob(theta, x, y_l)
    lp_w_ref = logprob(ref, x, y_w)
    lp_l_ref = logprob(ref, x, y_l)
    delta = beta * ((lp_w_theta - lp_w_ref) - (lp_l_theta - lp_l_ref))
    loss += -logsigmoid(delta)
loss /= batch_size
loss.backward()
optimizer.step()
```

注意：上面 `logprob` 需要做 token 层面的对数概率累加，并尽量在向量化运算中实现以提高效率。

## 9. 结论

DPO 是一个工程友好、数据高效的偏好优化方法。当你拥有高质量的偏好对并且希望简化训练管线时，DPO 能显著降低开发与训练复杂度；但在需要更强探索性或复杂回报建模的场景下，PPO 与基于奖励的 RLHF 仍然不可或缺。

本文给出了 DPO 的数学基础、实现要点与调优建议，并对比了 PPO 的关键差异。下一步可以基于本文的训练流程实现一个小规模试验，并用人工评估（human eval）与自动指标双轨验证训练效果。

## 参考文献

1. Rafailov, R., et al. "Direct preference optimization: Your language model is secretly a reward model." arXiv:2305.18290 (2023).
2. Schulman, J., et al. "Proximal policy optimization algorithms." arXiv:1707.06347 (2017).
