---
title: 大模型训练方法：GRPO 和 GSPO
date: 2025-11-18 16:20:00
update: 2025-11-18 16:20:00
categories: LLM
tags: [LLM, GRPO, GSPO, 强化学习]
---

GSPO (Group Sequence Policy Optimization) 作为 GRPO (Generalized Reinforcement Policy Optimization) 的升级版，通过将优化粒度从 "token 级" 提升到 "序列级"，从根本上解决了 GRPO 在训练大模型 (特别是 MoE 模型) 时的不稳定问题，同时保持了 GRPO 的轻量特性。

<!-- more -->

## 大模型强化学习

要理解 GRPO 和 GSPO，我们可以从 “大模型强化学习的核心目标” 说起：让模型生成的内容更符合人类偏好（比如更有用、更安全）。为了实现这个目标，需要一种 “策略优化算法” 来调整模型的生成策略 —— 而 GRPO 和 GSPO 都是这类算法，只是优化的 “视角” 和 “粒度” 不同。

先从基础概念入手：什么是 “策略优化”？

大模型生成文本的过程，本质是一个 “策略选择” 过程：比如用户问 “推荐一部电影”，模型需要从海量可能的回答中，选一个 “人类觉得最好” 的（比如 “《肖申克的救赎》，因为它探讨了希望与自由…”）。“策略优化” 的作用，就是告诉模型 “哪些回答更好”，并调整模型参数，让它下次更可能生成这类好回答。

这里的关键是 “如何衡量回答的好坏”—— 通常用 “奖励（Reward）” 表示：人类觉得好的回答，奖励高；不好的，奖励低。算法的任务就是 “根据奖励调整模型”，让模型多生成高奖励的回答。

### GRPO：逐个词调整的 "精细派"

GRPO 全称是Generalized Reinforcement Policy Optimization（广义强化策略优化），它是为了解决传统算法（比如 PPO）在大模型上 “计算量大、难训练” 的问题而设计的轻量版算法。

**核心思路：“逐词优化”（Token 级优化）**。大模型生成文本是 “逐词（Token）生成” 的（比如先想第一个词 “《肖申克”，再想第二个词 “的救赎》”，直到句子结束）。GRPO 的优化粒度和生成过程一致：针对每个词单独计算 “调整幅度”。

### GSPO：整体评价的 "全局派"

GSPO 全称是Group Sequence Policy Optimization（群组序列策略优化），它是 GRPO 的升级版，核心改进是把优化粒度从 “单个词” 提升到 “整个句子 / 序列”。

**核心思路：“整句优化”（序列级优化）**。GSPO 认为：“人类评价一个回答好不好，是看整个句子，而不是单个词”。因此，优化也应该从 “整体” 出发：
* 模型生成多个候选回答（比如针对 “推荐电影”，生成 3 个回答：A、B、C）；
* 给每个完整回答打分（比如 A 得 9 分，B 得 7 分，C 得 5 分）；
* 直接比较这些完整回答的 “整体好坏”，调整模型参数：让模型更倾向于生成高分数的完整序列（如 A），而不是纠结于某个词的调整。

## GRPO 和 GSPO 具体计算方式

上面是一个很宽泛的概念，我认为要想理解这两种方法，必须从具体例子出发来了解他们究竟是如何计算奖励的。

### GRPO 具体计算方式

GRPO 将整体奖励"分摊"到每个 token 并调整其生成概率的过程，核心基于**策略梯度 (Policy Gradient)** 的思想，同时结合了**重要性采样 (Importance Sampling)** 来稳定训练。具体计算可分为 3 个关键步骤，我们用你提到的例子"《肖申克的救赎》，很好看"（假设成序列为 $y = [y_1, y_2, y_3, y_4, y_5]$，其中 $y_1$=《，$y_2$=肖申克，$y_3$=的救赎》，$y_4$=，，$y_5$=很好看）来拆解：

#### 第一步：定义"每个 token 的奖励贡献"——用"累积奖励"分摊整体奖励

GRPO 认为，一个 token 的"贡献"不仅取决于它最终的整体奖励，还取决于它在序列中的位置：**越靠前的 token，对后续 token 的生成影响更大，因此需要承担更多奖励权重**。

具体通过**折扣累积奖励 (Discounted Return)** 计算每个 token 的贡献，公式为：

$$G_t = r_t + \gamma \cdot r_{t+1} + \gamma^2 \cdot r_{t+2} + \ldots + \gamma^{L-t} \cdot r_L$$

其中：

- $G_t$ 是第 $t$ 个 token 的"累积奖励贡献"；
- $r_t$ 是第 $t$ 步的即时奖励（大模型场景中，通常只有序列结束时才有整体奖励，即只有 $r_L = 整体奖励$，前面的 $r_1$ 到 $r_{L-1}$ 都为 0）；
- $\gamma$ 是折扣因子（通常取 0.95~0.99），作用是"让近期的 token（靠近奖励的 token）获得更高权重"。

**例子计算：**

假设整体奖励 $r_5 = 8$ 分（只有最后一个 token 有奖励），$\gamma = 0.9$，序列长度 $L = 5$：

- $G_5$（第 5 个 token "很好看"的贡献）：$r_5 = 8$（因为后面没有 token 了）；
- $G_4$（第 4 个 token "，"的贡献）：$\gamma \cdot r_5 = 0.9 \times 8 = 7.2$；
- $G_3$（第 3 个 token "的救赎）"的贡献）：$\gamma^2 \cdot r_5 = 0.9^2 \times 8 = 6.48$；
- $G_2$（第 2 个 token "肖申克"的贡献）：$\gamma^3 \cdot r_5 = 0.9^3 \times 8 \approx 5.83$；
- $G_1$（第 1 个 token "《"的贡献）：$\gamma^4 \cdot r_5 = 0.9^4 \times 8 \approx 5.25$。

这样，整体奖励 8 分就被"分摊"到了各个 token，且越靠近结尾的 token（如"很好看"）获得贡献值越高。

#### 第二步：计算"重要性比率"——衡量新旧策略的差异

GRPO 的核心是通过"重要性比率"判断：新策略（待更新的模型）生成某个 token 的概率，相比旧策略（更新前的模型）是否更优。

每个 token 的重要性比率公式为：

$$w_t = \frac{\pi_\theta(y_t|x, y_1 \ldots y_{t-1})}{\pi_{old}(y_t|x, y_1 \ldots y_{t-1})}$$

其中：

- $\pi_\theta$ 是当前模型（新策略）生成 token $y_t$ 的概率（给定输入 x 和前文 $y_1 \ldots y_{t-1}$）；
- $\pi_{old}$ 是更新前的模型（旧策略）生成同一个 token 的概率；
- $w_t$ 越大，说明新策略比旧策略更"倾向于"生成这个 token（在当前上下文下）。

**例子计算：**

假设输入 x 是"推荐一部电影"，旧模型和新模型对各个 token 的生成概率如下（简化为百分比）：

| token 位置 | token 内容 | 旧策略概率 $\pi_{old}$ | 新策略概率 $\pi_\theta$ | 重要性比率 $w_t$（新/旧） |
|------------|------------|------------------------|-------------------------|---------------------------|
| t = 1      | 《          | 30%                    | 40%                     | 40%/30% ≈ 1.33           |
| t = 2      | 肖申克      | 20%                    | 25%                     | 25%/20% = 1.25           |
| t = 3      | 的救赎）    | 15%                    | 18%                     | 18%/15% = 1.20           |
| t = 4      | ，          | 80%                    | 70%                     | 70%/80% = 0.875          |
| t = 5      | 很好看      | 40%                    | 60%                     | 60%/40% = 1.50           |

#### 第三步：计算梯度并更新模型——让"高贡献+高比率"的 token 更易被生成

GRPO 的最终目标是调整模型参数 $\theta$，使得"对奖励贡献高（$G_t$ 大）且新策略更倾向于生成（$w_t$ 大）"的 token 更易被生成，在未来被生成的概率更高。

梯度更新公式（简化版）为：

$$\nabla_\theta J(\theta) \approx \sum_{t=1}^{L} \left( \min(w_t, 1 + \epsilon) \cdot \max(w_t, 1 - \epsilon) \cdot G_t \cdot \nabla_\theta \log \pi_\theta(y_t|x, y_1 \ldots y_{t-1}) \right)$$

其中：

- $\nabla_\theta J(\theta)$ 是模型目标函数的梯度（即"调整方向"）；
- $\min(w_t, 1 + \epsilon) \cdot \max(w_t, 1 - \epsilon)$ 是"截断操作"（类似 PPO 的 clip），$\epsilon$ 常取 0.2，作用是限制 $w_t$ 的极端值（避免梯度爆炸）；
- $\nabla_\theta \log \pi_\theta(\ldots)$ 是"token 生成概率的对数梯度"，表示"如何调整参数能够提高该 token 的生成概率"。

**例子计算：**

结合前两步的结果（$\epsilon = 0.2$）：

1. 先对 $w_t$ 做截断：
   - $w_1 = 1.33 \rightarrow$ 截断到 $1 + \epsilon = 1.2$（因为 1.33>1.2）；
   - $w_2 = 1.25 \rightarrow$ 截断到 1.2；
   - $w_3 = 1.20 \rightarrow$ 保留 1.2；
   - $w_4 = 0.875 \rightarrow$ 截断到 $1 - \epsilon = 0.8$（因为 0.875<0.8）；
   - $w_5 = 1.50 \rightarrow$ 截断到 1.2。

2. 计算每个 token 的梯度权重（截断后 $w_t \times G_t$）：
   - t = 1: 1.2 × 5.25 ≈ 6.3；
   - t = 2: 1.2 × 5.83 ≈ 7.0；
   - t = 3: 1.2 × 6.48 ≈ 7.78；
   - t = 4: 0.8 × 7.2 ≈ 5.76；
   - t = 5: 1.2 × 8 = 9.6。

3. 模型参数会沿着这些权重的方向更新：
   - 权重为正且越大（如 t = 5 的 9.6），对应 token（"很好看"）的生成概率会被**显著提高**；
   - 权重较小（如 t = 4 的 5.76），对应 token（"，"）的生成概率会被**轻微提高**（因为权重为正）；
   - 如果某个 token 的权重为负（比如奖励为负时），其生成概率会被**降低**。

**总结：GRPO 的"token 级优化"逻辑**

1. 用**折扣累积奖励**将整体奖励分摊到各个 token，体现位置对奖励的影响；
2. 用**重要性比率**衡量新旧策略对同一token的改进；
3. 结合两者计算梯度，让**高贡献奖励且新策略更倾向于生成**的 token 更易被生成。

这种方式的优点是**贴合模型逐词生成的过程**，计算轻量；但缺点是**忽略序列整体逻辑**（比如某个 token 单独看贡献高，但组合起来不通顺），这也是 GSPO 转向**序列级优化**的核心原因。


### GSPO 具体计算方式

GSPO (Group Sequence Policy Optimization) 的核心是**跳过逐词分摊奖励，直接对完整序列做整体优化**，计算逻辑更接近人类对"回答好坏"的直观判断（即"整体评价"）。我们依然采用"推荐电影"的例子来拆解，假设模型生成了 3 个候选回答（序列），并得到人类打分（奖励），具体计算分为 4 步：

#### 第一步：生成候选序列并分组（核心是"对比好坏"）

GSPO 的优化依赖**序列之间的比较**，而非单个序列的孤立分析。具体来说：

1. 模型针对同一个输入（如"推荐一部电影"）生成多个候选回答（比如 3 个序列）；
2. 给每个完整序列打一个整体奖励分（人类或奖励模型评分）；
3. 按奖励高低将序列分成"好答案"（高奖励）和"差答案"（低奖励）。

**例子：**

输入："推荐一部电影"

生成 3 个候选序列及奖励：

- 序列 $y_A$："《肖申克的救赎》，它探讨了希望与自由，非常经典" → 奖励 $r_A = 9$ 分（好答案）
- 序列 $y_B$："《肖申克的救赎》，很好看" → 奖励 $r_B = 8$ 分（好答案）  
- 序列 $y_C$："《肖申克》，还行" → 奖励 $r_C = 5$ 分（差答案）

分组结果：好答案 = $\{y_A, y_B\}$，差答案 = $\{y_C\}$

#### 第二步：计算"序列级重要性比率"（衡量新旧策略对完整序列的偏好）

GSPO 不关心单个词的概率，只关注**新策略（待更新的模型）生成某个完整序列的概率**与**旧策略（更新前的模型）生成该序列的概率**的比值。由于为不同序列长度可能不同（比如 $y_A$ 长，$y_C$ 短），直接比概率会不公平（长序列概率天然更低），所以需要归一化处理。

**序列级重要性比率公式（核心创新）：**

$$s(y) = \left(\frac{\pi_\theta(y|x)}{\pi_{old}(y|x)}\right)^{\frac{1}{L}}$$

其中：

- $\pi_\theta(y|x)$ 是新策略生成完整序列 $y$ 的概率（即所有 token 概率的乘积：$\pi_\theta(y_1|x) \times \pi_\theta(y_2|x, y_1) \times \ldots \times \pi_\theta(y_L|x, y_1 \ldots y_{L-1})$）；
- $\pi_{old}(y|x)$ 是旧策略生成该序列的概率；
- $L$ 是序列的长度（token 数量）；
- 开 $L$ 次方根（$1/L$ 次方）是为了**归一化长度差异**，让不同长度的序列可以公平比较（避免长序列因概率乘积小而被低估）。

**例子计算：**

假设旧策略和新策略对 3 个序列的整体概率如下（简化为乘积结果）：

| 序列 | 长度 $L$ | 旧策略概率 $\pi_{old}(y\|x)$ | 新策略概率 $\pi_\theta(y\|x)$ | 比率（新/旧）| 归一化比率 $s(y)$（开 $L$ 次方）|
|------|----------|------------------------------|-------------------------------|-------------|--------------------------------|
| $y_A$ | 7 | $10^{-9}$（因长度长，概率低）| $2 \times 10^{-9}$ | 2 | $2^{1/7} \approx 1.104$ |
| $y_B$ | 5 | $10^{-7}$（长度中等，概率较高）| $1.5 \times 10^{-7}$ | 1.5 | $1.5^{1/5} \approx 1.084$ |
| $y_C$ | 3 | $10^{-5}$（长度短，概率高）| $0.8 \times 10^{-5}$ | 0.8 | $0.8^{1/3} \approx 0.928$ |

可以看到：归一化后，长序列 $y_A$ 的比率（1.104）没有被长度拖累，能和短序列公平比较。

#### 第三步：计算"群组优势"（明确模型该向哪些序列学习）

GSPO 的核心是让模型**多学好答案的序列，少学差答案的序列**，因此需要计算**好答案相对于差答案的优势**。

**群组优势公式：**

$$A = \frac{1}{|G_{good}|} \sum_{y \in G_{good}} s(y) - \frac{1}{|G_{bad}|} \sum_{y \in G_{bad}} s(y)$$

其中：

- $G_{good}$ 是好答案集合，$|G_{good}|$ 是好答案数量；
- $G_{bad}$ 是差答案集合，$|G_{bad}|$ 是差答案数量；
- $A$ 为正，说明新策略目前更倾向生成好答案序列（方向正确）；$A$ 为负，则需要调整。

**例子计算：**

好答案有 2 个序列，差答案有 1 个：

$$A = \frac{s(y_A) + s(y_B)}{2} - s(y_C) = \frac{1.104 + 1.084}{2} - 0.928 \approx 1.094 - 0.928 = 0.166$$

$A = 0.166 > 0$，说明新策略当前对好答案的偏好优于差答案，是积极信号。

#### 第四步：计算梯度并更新模型（让好答案序列更易被生成）

GSPO 的梯度更新目标是：**提高好答案序列的生成概率，降低差答案序列的生成概率**，且更新幅度由**群组优势**和**序列重要性比率**共同决定。

**梯度更新公式（简化版）：**

$$\nabla_\theta J(\theta) \approx \left(\frac{1}{|G_{good}|} \sum_{y \in G_{good}} s(y) \cdot \nabla_\theta \log \pi_\theta(y|x)\right) - \left(\frac{1}{|G_{bad}|} \sum_{y \in G_{bad}} s(y) \cdot \nabla_\theta \log \pi_\theta(y|x)\right)$$

其中：

- $\nabla_\theta \log \pi_\theta(y|x)$ 是**完整序列生成概率的对数梯度**，表示**如何调整参数能够提高该序列的整体生成概率**；
- 公式本质是**好答案的平均梯度**减去**差答案的平均梯度**，强制模型向好答案的方向优化。

**例子计算：**

假设 3 个序列的对数梯度分别为（简化值）：

- $\nabla \log \pi(y_A|x) = 5$（调整参数能显著提高 $y_A$ 的概率）；
- $\nabla \log \pi(y_B|x) = 3$（调整参数能中等提高 $y_B$ 的概率）；
- $\nabla \log \pi(y_C|x) = 2$（调整参数能轻微提高 $y_C$ 的概率）。

则梯度为：

$$\nabla J(\theta) \approx \left(\frac{1.104 \times 5 + 1.084 \times 3}{2}\right) - (0.928 \times 2)$$

$$\approx \left(\frac{5.52 + 3.252}{2}\right) - 1.856 \approx 4.386 - 1.856 = 2.53$$

梯度为正，说明模型参数会沿着这个方向更新，最终效果是：

- $y_A$ 和 $y_B$（好答案）的整体生成概率会**显著提高**；
- $y_C$（差答案）的生成概率会**相对降低**。

**总结：GSPO 的"序列级优化"逻辑**

1. **不折分奖励**：直接用完整序列的整体奖励分组（好/差），避免 GRPO 中"奖励分摊到 token"的逻辑矛盾；
2. **序列级比率**：通过几何平均（开长度次方）归一化不同长度序列的概率，公平比较；  
3. **群组对比学习**：用**好答案平均梯度 - 差答案平均梯度**更新模型，强制模型向整体优质的序列学习。

这种方式的核心优势是：**优化目标与人类评价逻辑完全一致**（关注整体），因此生成的序列更连贯，且在长文本、MoE 模型中更稳定（无需处理 token 级波动）。

