---
title: LLM 训练：PPO 和 DPO
date: 2025-06-24 20:44:51
update: 2025-06-24 20:44:51
categories: LLM
tags: [LLM, rlhf, ppo, dpo]
mathjax: true
---

已经接近 3 年没有更新博客了。今天立下一个 flag，开始准备 LLM 面试知识，主要是八股文为主，想到哪写到哪。第一篇没想到写啥，觉得对 PPO 和 DPO 比较了解，就先直接写这个吧。

<!-- more -->

## 引言

在大语言模型（LLM）的训练过程中，RLHF（Reinforcement Learning from Human Feedback）是一个重要的技术，它通过人类反馈来优化模型的行为。在 RLHF 中，PPO（Proximal Policy Optimization）和 DPO（Direct Preference Optimization）是两种主流的算法。本文将详细介绍这两种算法的工作原理、数学推导以及它们之间的区别。

## PPO（Proximal Policy Optimization）

### PPO 基本原理

PPO 是一种基于策略梯度的强化学习算法，它的核心思想是通过限制策略更新的步长来保证训练的稳定性。在 RLHF 中，PPO 被用来优化语言模型，使其生成更符合人类偏好的回答。

### PPO 的数学推导

#### 1. 策略梯度定理

首先，我们回顾一下策略梯度定理。对于策略 $\pi_\theta$，目标函数为：

$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)]
$$

其中 $\tau$ 是轨迹，$R(\tau)$ 是轨迹的奖励。

**策略梯度定理**是强化学习中的一个核心定理，它告诉我们如何直接优化策略参数 $\theta$ 来最大化期望奖励。这个定理的重要性在于：

1. **直接优化策略**：不像价值函数方法需要先学习价值函数再推导策略，策略梯度方法直接优化策略参数
2. **理论基础**：为所有基于策略的强化学习算法提供了数学基础
3. **适用性广**：适用于连续动作空间和离散动作空间

策略梯度定理告诉我们：

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [\nabla_\theta \log \pi_\theta(\tau) R(\tau)]
$$

这个公式的含义是：

- **左侧**：目标函数 $J(\theta)$ 关于参数 $\theta$ 的梯度
- **右侧**：策略对数概率的梯度与奖励的乘积的期望

**直观理解**：

- 如果某个轨迹 $\tau$ 的奖励 $R(\tau)$ 很高，我们就增加这个轨迹的概率
- 如果某个轨迹 $\tau$ 的奖励 $R(\tau)$ 很低，我们就减少这个轨迹的概率
- $\nabla_\theta \log \pi_\theta(\tau)$ 告诉我们在参数空间中应该朝哪个方向移动

**实际应用中的问题**：

1. **高方差**：直接使用这个公式会导致训练不稳定
2. **样本效率低**：需要大量样本来估计期望
3. **更新步长难以控制**：可能导致策略更新过大或过小

这就是为什么需要 PPO 等改进算法的原因。

#### 2. PPO 的目标函数

PPO 通过引入一个比率项来限制策略更新的幅度：

$$
r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}
$$

**比率项的含义**：

- $\pi_{\theta_{old}}(a_t|s_t)$ 是旧策略（更新前）在状态 $s_t$ 下选择动作 $a_t$ 的概率
- $\pi_\theta(a_t|s_t)$ 是新策略（更新后）在状态 $s_t$ 下选择动作 $a_t$ 的概率
- 比率 $r_t(\theta)$ 衡量了新策略相对于旧策略的变化程度

**为什么需要限制策略更新幅度**：

1. **防止策略崩溃**：如果策略更新过大，可能导致某些动作的概率变为 0，失去探索能力
2. **保证训练稳定性**：过大的更新步长会导致训练不稳定，甚至发散
3. **避免灾难性遗忘**：防止新策略完全偏离旧策略，丢失之前学到的有用知识

PPO 的目标函数为：

$$
L^{CLIP}(\theta) = \mathbb{E}_t [\min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t)]
$$

这个公式的核心思想是：

- 如果 $A_t > 0$（好的动作），我们希望增加这个动作的概率，但最多只能增加到 $(1+\epsilon)$ 倍
- 如果 $A_t < 0$（坏的动作），我们希望减少这个动作的概率，但最多只能减少到 $(1-\epsilon)$ 倍
- $\epsilon$ 通常设置为 0.2，意味着策略更新幅度被限制在 ±20% 以内

其中：

- $A_t$ 是优势函数（Advantage function）
- $\epsilon$ 是裁剪参数，通常设置为 0.2
- $\text{clip}(x, a, b)$ 函数将 $x$ 限制在 $[a, b]$ 范围内

#### 3. 优势函数

优势函数衡量了某个动作相对于平均水平的优势：

$$
A_t = Q(s_t, a_t) - V(s_t)
$$

其中 $Q(s_t, a_t)$ 是动作价值函数，$V(s_t)$ 是状态价值函数。

**价值函数的基本概念**：

在强化学习中，价值函数用于评估状态或状态-动作对的价值，帮助我们做出更好的决策。

**状态价值函数 $V(s_t)$**：
- **定义**：在状态 $s_t$ 下，遵循策略 $\pi$ 的期望累积奖励
- **数学表达式**：
  $$
  V^\pi(s_t) = \mathbb{E}_{\pi} [\sum_{k=0}^{\infty} \gamma^k R_{t+k} | S_t = s_t]
  $$
- **含义**：表示从状态 $s_t$ 开始，按照策略 $\pi$ 行动，能够获得的期望总奖励
- **特点**：只依赖于状态，不依赖于具体动作

**动作价值函数 $Q(s_t, a_t)$**：
- **定义**：在状态 $s_t$ 下采取动作 $a_t$，然后遵循策略 $\pi$ 的期望累积奖励
- **数学表达式**：
  $$
  Q^\pi(s_t, a_t) = \mathbb{E}_{\pi} [\sum_{k=0}^{\infty} \gamma^k R_{t+k} | S_t = s_t, A_t = a_t]
  $$
- **含义**：表示在状态 $s_t$ 下采取动作 $a_t$，然后按照策略 $\pi$ 行动，能够获得的期望总奖励
- **特点**：依赖于状态和动作的组合

**优势函数 $A_t$ 的作用**：
- **相对评估**：优势函数衡量了某个动作相对于该状态下所有动作平均水平的优势
- **决策指导**：
  - 如果 $A_t > 0$，说明动作 $a_t$ 比平均水平好，应该增加其概率
  - 如果 $A_t < 0$，说明动作 $a_t$ 比平均水平差，应该减少其概率
  - 如果 $A_t = 0$，说明动作 $a_t$ 处于平均水平

**在 PPO 中的重要性**：
1. **减少方差**：相比直接使用奖励，优势函数提供了更稳定的学习信号
2. **基线作用**：状态价值函数作为基线，减少了策略梯度的方差
3. **相对比较**：通过相对比较而不是绝对奖励，使得训练更加稳定

**实际计算中的挑战**：
- 真实的价值函数通常是未知的，需要通过神经网络来估计
- 这就是为什么 PPO 需要 Critic 模型来估计状态价值函数
- 优势函数通常通过时序差分（TD）方法或其他技术来估计

### PPO 在 RLHF 中的应用

在 RLHF 中，PPO 需要四个模型：

1. **Actor Model**：被训练的策略模型
2. **Critic Model**：价值函数模型，用于估计状态价值
3. **Reward Model**：奖励模型，用于计算即时奖励
4. **Reference Model**：参考模型，用于防止策略偏离太远

关于这 4 个模型，可以参考我之前的文章：[大模型 RLHF 训练中的 PPO 算法细节](https://murphypei.github.io/blog/2024/07/llm-rlhf-ppo.html)

**四个模型的作用和特点**：

**Actor Model（策略模型）**：
- 这是我们要训练的主要模型，最终用于实际应用
- 接收 prompt，生成 response
- 在训练过程中，其参数会不断更新以优化策略

**Critic Model（价值函数模型）**：
- 用于估计状态价值函数 $V(s_t)$
- 通常用 Reward Model 初始化，架构与 Actor 相似
- 在最后一层增加 Value Head，输出单一的价值估计
- 需要更新参数，因为价值估计能力需要不断提升

**Reward Model（奖励模型）**：
- 计算即时奖励 $R_t$，评估当前 response 的好坏
- 参数固定不更新，作为客观的评估标准
- 只关心当前 response 的质量，不考虑长期影响

**Reference Model（参考模型）**：
- 通常用 SFT 模型初始化，参数冻结
- 主要作用是防止 Actor"训歪"，避免过拟合到高分但无意义的回答
- 通过 KL 散度约束，确保新策略与参考策略的输出分布相似

**为什么需要四个模型**：
1. **Actor**：学习生成符合人类偏好的回答
2. **Critic**：评估整体价值，减少训练方差
3. **Reward**：提供客观的即时评估标准
4. **Reference**：防止策略偏离太远，保持语言能力

PPO 的损失函数包括三个部分：

$$
L_{PPO} = L^{CLIP} - \alpha L^{KL} + \beta L^{VF}
$$

其中：
- $L^{CLIP}$ 是 PPO 的主要损失，通过裁剪机制限制策略更新
- $L^{KL}$ 是 KL 散度损失，用于限制与参考模型的差异
- $L^{VF}$ 是价值函数损失，用于训练 Critic 模型
- $\alpha$ 和 $\beta$ 是权重参数，平衡不同损失项的重要性

**训练流程**：
1. Actor 接收 prompt，生成 response
2. Reward Model 计算即时奖励
3. Critic Model 估计状态价值
4. 计算优势函数 $A_t = R_t - V_t$
5. 使用 PPO 损失函数更新 Actor 和 Critic 参数
6. 通过 KL 散度约束确保与 Reference Model 的相似性

## DPO（Direct Preference Optimization）

### DPO 基本原理

DPO 是一种更直接的方法，它不需要显式的奖励模型，而是直接通过人类偏好数据来优化策略。DPO 的核心思想是将偏好学习问题转化为一个分类问题。

**DPO 的核心创新**：
1. **消除奖励模型**：不需要单独训练奖励模型，简化了训练流程
2. **直接偏好学习**：直接从人类偏好数据中学习，避免了奖励建模的误差
3. **理论等价性**：证明了 DPO 与基于奖励模型的 RLHF 在理论上是等价的

### DPO 的数学推导

#### 1. 偏好学习问题

给定一个提示 $x$ 和两个回答 $y_w$（获胜）和 $y_l$（失败），我们的目标是学习一个策略 $\pi_\theta$，使得：

$$
P(y_w \succ y_l | x) > P(y_l \succ y_w | x)
$$

**偏好数据的含义**：
- $y_w \succ y_l$ 表示在提示 $x$ 下，回答 $y_w$ 比 $y_l$ 更受人类偏好
- 这种偏好关系反映了人类的价值判断，是 RLHF 的核心数据

#### 2. Bradley-Terry 模型

DPO 使用 Bradley-Terry 模型来建模偏好。这个模型假设偏好概率与奖励函数之间存在以下关系：

$$
P(y_w \succ y_l | x) = \frac{\exp(r_\theta(x, y_w))}{\exp(r_\theta(x, y_w)) + \exp(r_\theta(x, y_l))}
$$

其中 $r_\theta(x, y)$ 是奖励函数。

**Bradley-Terry 模型的特点**：
- **单调性**：奖励越高，被偏好的概率越大
- **对称性**：$P(y_w \succ y_l | x) + P(y_l \succ y_w | x) = 1$
- **温度控制**：可以通过调整指数函数的温度参数来控制偏好强度

#### 3. 从奖励函数到策略的映射

DPO 的关键洞察是：我们可以将奖励函数表示为策略与参考策略的比值：

$$
r_\theta(x, y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}
$$

其中：
- $\pi_{ref}$ 是参考策略（通常是 SFT 模型）
- $\beta$ 是温度参数，控制奖励的强度

**这个映射的物理意义**：
- 如果 $\pi_\theta(y|x) > \pi_{ref}(y|x)$，说明新策略更倾向于生成回答 $y$，奖励为正
- 如果 $\pi_\theta(y|x) < \pi_{ref}(y|x)$，说明新策略不太倾向于生成回答 $y$，奖励为负
- $\beta$ 控制奖励的敏感度，值越大，策略差异对奖励的影响越明显

#### 4. DPO 的目标函数

将奖励函数代入 Bradley-Terry 模型，得到 DPO 的目标函数：

$$
L_{DPO} = -\mathbb{E}_{(x, y_w, y_l) \sim D} \left[\log \sigma\left(\beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\right)\right]
$$

其中：
- $\sigma$ 是 sigmoid 函数：$\sigma(x) = \frac{1}{1 + e^{-x}}$
- $\beta$ 是温度参数，通常设置为 0.1-0.5
- $\pi_{ref}$ 是参考策略（通常是 SFT 模型）

**目标函数的直观理解**：
- 我们希望最大化偏好数据的对数似然
- 对于偏好对 $(y_w, y_l)$，我们希望 $P(y_w \succ y_l | x)$ 尽可能大
- 这等价于让 $\beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}$ 尽可能大
- 即让获胜回答相对于参考策略的提升幅度大于失败回答

#### 5. 奖励函数的推导

通过 DPO 的训练，我们可以推导出隐含的奖励函数：

$$
r_\theta(x, y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}
$$

这个公式表明，DPO 实际上是在学习一个相对于参考策略的奖励函数。

**奖励函数的性质**：
1. **相对性**：奖励是相对于参考策略定义的，不是绝对奖励
2. **可解释性**：奖励直接反映了策略相对于参考策略的偏好程度
3. **一致性**：与人类偏好数据保持一致

### DPO 的训练过程

**训练步骤**：
1. **数据准备**：收集人类偏好数据 $(x, y_w, y_l)$
2. **策略初始化**：用 SFT 模型初始化 $\pi_\theta$ 和 $\pi_{ref}$
3. **前向传播**：计算 $\pi_\theta(y_w|x)$ 和 $\pi_\theta(y_l|x)$
4. **损失计算**：使用 DPO 损失函数计算梯度
5. **参数更新**：只更新 $\pi_\theta$，保持 $\pi_{ref}$ 固定

**关键超参数**：
- **$\beta$（温度参数）**：控制策略更新的强度，值越大更新越激进
- **学习率**：控制参数更新的步长
- **批次大小**：影响训练的稳定性和效率

### DPO 损失计算的具体步骤

为了更好地理解 DPO 的训练过程，我们来详细拆解针对一个具体样本的损失计算步骤。整个过程的核心思想是：**直接利用偏好数据（哪个回答更好，哪个更差）来调整模型，让模型生成"更好"回答的概率变高，生成"更差"回答的概率变低。**

为了实现这个目标，DPO 的训练过程涉及两个模型：
1. **策略模型 ($\pi_{\theta}$)**：我们正在训练和优化的模型。它的参数在训练中会不断更新。
2. **参考模型 ($\pi_{ref}$)**：一个固定的、不参与训练的模型。通常是策略模型在 DPO 训练开始前的初始版本（比如，经过 SFT 监督微调后的模型）。它的作用是作为一把"尺子"，防止策略模型在学习偏好的过程中偏离太远，忘掉其原有的语言能力。

假设我们有以下一个训练样本：
- **Prompt (x)**: "请介绍一下长城"
- **Chosen (y_w)**: "长城是古代中国为抵御侵略而修筑的军事工程。" (被标注为更好的回答)
- **Rejected (y_l)**: "长城是个墙，在中国。" (被标注为更差的回答)

#### 第一步：计算两个回答在两个模型下的概率

模型处理的是 token 序列，而不是文字。假设经过分词后，两个回答的 token 序列如下：
- **y_w**: `["长城", "是", "古代", "中国", "为", ...]`
- **y_l**: `["长城", "是", "个", "墙", "，", ...]`

对于一个自回归语言模型来说，一个完整序列的概率是该序列中每个 token 的条件概率的乘积。在实际计算中，为了数值稳定性，我们通常使用对数概率（log probabilities）的加和。

**计算过程：**

1. **对于 "Chosen" 回答 (y_w):**
   - 将 `Prompt (x)` 和 `Chosen (y_w)` 拼接起来，输入给**策略模型 ($\pi_{\theta}$)**。
   - 模型会为 `y_w` 中的每一个 token 计算其生成的对数概率。例如，计算 P("是" | "长城")，P("古代" | "长城是")，以此类推。
   - 将 `y_w` 序列中所有 token 的对数概率相加，得到**策略模型**认为生成 `y_w` 的总对数概率：$logP_{\pi_{\theta}}(y_w|x)$。
   - 用同样的方法，将 `Prompt (x)` 和 `Chosen (y_w)` 输入给**参考模型 ($\pi_{ref}$)**，计算出**参考模型**认为生成 `y_w` 的总对数概率：$logP_{\pi_{ref}}(y_w|x)$。

2. **对于 "Rejected" 回答 (y_l):**
   - 同样地，将 `Prompt (x)` 和 `Rejected (y_l)` 拼接后，分别输入给**策略模型 ($\pi_{\theta}$)** 和**参考模型 ($\pi_{ref}$)**。
   - 计算出策略模型生成 `y_l` 的总对数概率：$logP_{\pi_{\theta}}(y_l|x)$。
   - 计算出参考模型生成 `y_l` 的总对数概率：$logP_{\pi_{ref}}(y_l|x)$。

经过这一步，我们就得到了四个核心的对数概率值。

#### 第二步：计算隐式奖励 (Implicit Reward) 或偏好度

DPO 的精髓在于，它证明了模型的偏好程度可以被一个简单的公式表示，这个公式衡量了策略模型相对于参考模型的改进程度。

1. **计算 "Chosen" 回答的偏好度：**
   这个值反映了策略模型相比于参考模型，有多"倾向于"生成那个更好的回答。
   $$r_w = \beta \cdot (logP_{\pi_{\theta}}(y_w|x) - logP_{\pi_{ref}}(y_w|x))$$
   其中 $\beta$ 是一个超参数（通常设为 0.1），用来控制策略模型与参考模型之间的差异程度。

2. **计算 "Rejected" 回答的偏好度：**
   同理，这个值反映了策略模型相比于参考模型，有多"倾向于"生成那个更差的回答。
   $$r_l = \beta \cdot (logP_{\pi_{\theta}}(y_l|x) - logP_{\pi_{ref}}(y_l|x))$$

#### 第三步：计算最终的 DPO 损失

DPO 的损失函数目标是最大化"Chosen"回答的偏好度与"Rejected"回答的偏好度之间的差距。

1. **计算偏好度差异：**
   $$\text{diff} = r_w - r_l$$
   将第二步的公式代入，得到：
   $$\text{diff} = \beta \cdot [(logP_{\pi_{\theta}}(y_w|x) - logP_{\pi_{ref}}(y_w|x)) - (logP_{\pi_{\theta}}(y_l|x) - logP_{\pi_{ref}}(y_l|x))]$$

2. **应用 Sigmoid 函数和负对数：**
   DPO 将这个差异值传入一个 `log-sigmoid` 函数中来构造最终的损失。
   $$\text{Loss} = -log(\sigma(\text{diff}))$$
   其中 $\sigma$ 是 Sigmoid 函数。

**这个损失函数的直观理解是：**
- 如果偏好度差异 `diff` 很大（即策略模型非常明确地更喜欢 `y_w` 而不是 `y_l`），那么 $\sigma(\text{diff})$ 的值会趋近于 1，$log(\sigma(\text{diff}))$ 会趋近于 0，最终的损失值 `Loss` 也就很小。这表示模型已经学习得很好了，不需要太多调整。
- 如果偏好度差异 `diff` 很小甚至是负数（即策略模型对 `y_w` 和 `y_l` 的偏好不明显，甚至搞反了），那么 $\sigma(\text{diff})$ 的值会小于 1，$log(\sigma(\text{diff}))$ 会是一个负数，最终的损失值 `Loss` 就会是一个较大的正数。这个较大的损失会通过反向传播来更新**策略模型**的参数。

#### 总结

通过这个损失函数进行梯度下降，模型参数的更新会朝着以下目标进行：
- **提高** $logP_{\pi_{\theta}}(y_w|x)$ (增加生成 Chosen 回答的概率)
- **降低** $logP_{\pi_{\theta}}(y_l|x)$ (降低生成 Rejected 回答的概率)

同时，由于参考模型 $logP_{\pi_{ref}}$ 的存在，这个过程又被施加了一个约束，确保策略模型不会为了迎合偏好而产生乱七八糟、不合语法的回答，从而保证了训练的稳定性。这就是 DPO 针对一个 token 序列（样本）计算输出和损失的全过程。

### DPO 的优势和局限性

**优势**：
1. **训练简单**：只需要两个模型，训练流程简单
2. **数据效率高**：直接使用偏好数据，避免了奖励建模的误差
3. **理论保证**：在理论上与基于奖励的 RLHF 等价
4. **计算效率高**：单轮训练，不需要复杂的优势估计

**局限性**：
1. **依赖参考策略**：奖励函数是相对于参考策略定义的
2. **偏好数据质量**：对偏好数据的质量要求较高
3. **探索能力有限**：可能无法探索到远离参考策略的新策略
4. **温度参数敏感**：$\beta$ 的选择对性能影响较大

### DPO 与 PPO 的理论联系

**等价性的核心前提**：

DPO 与 PPO 等价的核心前提是：**奖励函数必须满足特定的形式**。具体来说，奖励函数必须能够表示为：

$$
r(x, y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}
$$

这个前提条件意味着：

1. **奖励函数与策略的耦合**：奖励函数不能是任意的，必须与当前策略 $\pi_\theta$ 和参考策略 $\pi_{ref}$ 相关
2. **相对性**：奖励是相对于参考策略定义的，不是绝对奖励
3. **策略依赖性**：奖励函数会随着策略的更新而变化

**为什么这个前提很重要**：

在实际的 RLHF 中，奖励函数通常是独立训练的，其形式为 $r(x, y) = f_\phi(x, y)$，其中 $f_\phi$ 是一个独立的神经网络。这种形式的奖励函数与 DPO 假设的形式完全不同。

**等价性的假设前提**：

DPO 与 PPO 的等价性是在以下关键假设下成立的：

1. **奖励函数假设**：奖励函数必须满足 $r(x, y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}$ 的形式
2. **优势函数简化**：忽略价值函数，直接使用奖励作为优势函数，即 $A_t = r_t$
3. **策略约束方式**：使用 KL 散度约束而不是 PPO 的裁剪约束
4. **单步优化**：假设每次更新都是单步的，不考虑多步交互

**理论背景**：

这个等价性来自于**奖励函数与策略之间的对偶关系**。在强化学习中，存在一个重要的理论结果：对于任何奖励函数 $r(x, y)$，都存在一个最优策略 $\pi^*(y|x)$，使得：

$$
\pi^*(y|x) \propto \pi_{ref}(y|x) \exp(\frac{r(x, y)}{\beta})
$$

这个关系表明，奖励函数和策略之间存在一一对应的关系。DPO 的关键洞察是：如果我们直接学习策略，就可以隐式地学习到对应的奖励函数。

**等价性证明的局限性**：

需要注意的是，这种等价性有以下局限性：

1. **奖励函数形式限制**：只有在特定形式的奖励函数下才成立
2. **忽略价值函数**：实际 PPO 中价值函数的作用被简化了
3. **约束方式不同**：PPO 的裁剪约束和 DPO 的 KL 散度约束在理论上不等价
4. **训练稳定性**：虽然理论等价，但实际训练中的稳定性可能不同

**实际应用中的差异**：

尽管在理论上存在等价性，但在实际应用中：

1. **PPO**：通过显式奖励模型提供更直接的监督信号
2. **DPO**：通过偏好数据提供相对比较信号
3. **训练稳定性**：PPO 的裁剪机制可能提供更好的训练稳定性
4. **探索能力**：PPO 可能具有更好的探索能力

**等价性证明**：
DPO 可以看作是 PPO 在特定条件下的简化版本。当 PPO 中的：
- 奖励模型 $r(x, y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}$
- 优势函数 $A_t = r_t$（忽略价值函数）
- 策略约束通过 KL 散度实现

此时，PPO 的损失函数就退化为 DPO 的形式。

**主要区别**：
1. **奖励建模**：PPO 需要显式奖励模型，DPO 隐含在策略中
2. **优势估计**：PPO 需要复杂的优势估计，DPO 直接使用奖励
3. **约束方式**：PPO 使用裁剪约束，DPO 使用 KL 散度约束

## PPO 和 DPO 的区别

### 1. 训练复杂度

- **PPO**：需要四个模型（Actor、Critic、Reward、Reference），训练过程复杂
- **DPO**：只需要两个模型（策略模型和参考模型），训练过程简单

### 2. 数据需求

- **PPO**：需要显式的奖励信号或奖励模型
- **DPO**：只需要偏好数据（哪个更好），不需要显式奖励

### 3. 计算效率

- **PPO**：需要多轮交互和复杂的优势估计
- **DPO**：单轮训练，计算效率更高

### 4. 稳定性

- **PPO**：通过裁剪机制保证训练稳定性
- **DPO**：通过 KL 散度约束保证稳定性

### 5. 适用场景

- **PPO**：适用于有明确奖励信号或可以训练奖励模型的场景
- **DPO**：适用于只有人类偏好数据的场景

## 总结

PPO 和 DPO 都是 RLHF 中的重要算法，它们各有优缺点：

- **PPO** 更加成熟和稳定，但训练复杂度高
- **DPO** 更加简单和高效，但可能在某些场景下效果不如 PPO

选择哪种算法主要取决于具体的应用场景和可用的数据。在实际应用中，可以根据需求选择合适的算法，或者将两种算法结合使用。

## 参考文献

1. Schulman, J., et al. "Proximal policy optimization algorithms." arXiv preprint arXiv:1707.06347 (2017).
2. Rafailov, R., et al. "Direct preference optimization: Your language model is secretly a reward model." arXiv preprint arXiv:2305.18290 (2023).
