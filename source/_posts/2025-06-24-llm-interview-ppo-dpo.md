---
title: LLM 八股文：PPO 和 DPO
date: 2025-06-24 20:44:51
update: 2025-06-24 20:44:51
categories: LLM
tags: [LLM, 面试, rlhf, ppo, dpo]
mathjax: true
---
已经接近 3 年没有更新博客了。今天立下一个 flag，开始准备 LLM 面试知识，主要是八股文为主，想到哪写到哪。第一篇没想到写啥，觉得对 PPO 和 DPO 比较了解，就先直接写这个吧。

<!-- more -->

## 引言

在大语言模型（LLM）的训练过程中，RLHF（Reinforcement Learning from Human Feedback）是一个重要的技术，它通过人类反馈来优化模型的行为。在RLHF中，PPO（Proximal Policy Optimization）和DPO（Direct Preference Optimization）是两种主流的算法。本文将详细介绍这两种算法的工作原理、数学推导以及它们之间的区别。

## PPO（Proximal Policy Optimization）

### PPO 基本原理

PPO 是一种基于策略梯度的强化学习算法，它的核心思想是通过限制策略更新的步长来保证训练的稳定性。在RLHF中，PPO被用来优化语言模型，使其生成更符合人类偏好的回答。

### PPO 的数学推导

#### 1. 策略梯度定理

首先，我们回顾一下策略梯度定理。对于策略 $\pi_\theta$，目标函数为：

$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)]
$$

其中 $\tau$ 是轨迹，$R(\tau)$ 是轨迹的奖励。

**策略梯度定理**是强化学习中的一个核心定理，它告诉我们如何直接优化策略参数 $\theta$ 来最大化期望奖励。这个定理的重要性在于：

1. **直接优化策略**：不像价值函数方法需要先学习价值函数再推导策略，策略梯度方法直接优化策略参数
2. **理论基础**：为所有基于策略的强化学习算法提供了数学基础
3. **适用性广**：适用于连续动作空间和离散动作空间

策略梯度定理告诉我们：

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [\nabla_\theta \log \pi_\theta(\tau) R(\tau)]
$$

这个公式的含义是：

- **左侧**：目标函数 $J(\theta)$ 关于参数 $\theta$ 的梯度
- **右侧**：策略对数概率的梯度与奖励的乘积的期望

**直观理解**：

- 如果某个轨迹 $\tau$ 的奖励 $R(\tau)$ 很高，我们就增加这个轨迹的概率
- 如果某个轨迹 $\tau$ 的奖励 $R(\tau)$ 很低，我们就减少这个轨迹的概率
- $\nabla_\theta \log \pi_\theta(\tau)$ 告诉我们在参数空间中应该朝哪个方向移动

**实际应用中的问题**：

1. **高方差**：直接使用这个公式会导致训练不稳定
2. **样本效率低**：需要大量样本来估计期望
3. **更新步长难以控制**：可能导致策略更新过大或过小

这就是为什么需要 PPO 等改进算法的原因。

#### 2. PPO 的目标函数

PPO 通过引入一个比率项来限制策略更新的幅度：

$$
r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}
$$

**比率项的含义**：

- $\pi_{\theta_{old}}(a_t|s_t)$ 是旧策略（更新前）在状态 $s_t$ 下选择动作 $a_t$ 的概率
- $\pi_\theta(a_t|s_t)$ 是新策略（更新后）在状态 $s_t$ 下选择动作 $a_t$ 的概率
- 比率 $r_t(\theta)$ 衡量了新策略相对于旧策略的变化程度

**为什么需要限制策略更新幅度**：

1. **防止策略崩溃**：如果策略更新过大，可能导致某些动作的概率变为0，失去探索能力
2. **保证训练稳定性**：过大的更新步长会导致训练不稳定，甚至发散
3. **避免灾难性遗忘**：防止新策略完全偏离旧策略，丢失之前学到的有用知识

PPO 的目标函数为：

$$
L^{CLIP}(\theta) = \mathbb{E}_t [\min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t)]
$$

这个公式的核心思想是：

- 如果 $A_t > 0$（好的动作），我们希望增加这个动作的概率，但最多只能增加到 $(1+\epsilon)$ 倍
- 如果 $A_t < 0$（坏的动作），我们希望减少这个动作的概率，但最多只能减少到 $(1-\epsilon)$ 倍
- $\epsilon$ 通常设置为 0.2，意味着策略更新幅度被限制在 ±20% 以内

其中：

- $A_t$ 是优势函数（Advantage function）
- $\epsilon$ 是裁剪参数，通常设置为 0.2
- $\text{clip}(x, a, b)$ 函数将 $x$ 限制在 $[a, b]$ 范围内

#### 3. 优势函数

优势函数衡量了某个动作相对于平均水平的优势：

$$
A_t = Q(s_t, a_t) - V(s_t)
$$

其中 $Q(s_t, a_t)$ 是动作价值函数，$V(s_t)$ 是状态价值函数。

**价值函数的基本概念**：

在强化学习中，价值函数用于评估状态或状态-动作对的价值，帮助我们做出更好的决策。

**状态价值函数 $V(s_t)$**：
- **定义**：在状态 $s_t$ 下，遵循策略 $\pi$ 的期望累积奖励
- **数学表达式**：
  $$
  V^\pi(s_t) = \mathbb{E}_{\pi} [\sum_{k=0}^{\infty} \gamma^k R_{t+k} | S_t = s_t]
  $$
- **含义**：表示从状态 $s_t$ 开始，按照策略 $\pi$ 行动，能够获得的期望总奖励
- **特点**：只依赖于状态，不依赖于具体动作

**动作价值函数 $Q(s_t, a_t)$**：
- **定义**：在状态 $s_t$ 下采取动作 $a_t$，然后遵循策略 $\pi$ 的期望累积奖励
- **数学表达式**：
  $$
  Q^\pi(s_t, a_t) = \mathbb{E}_{\pi} [\sum_{k=0}^{\infty} \gamma^k R_{t+k} | S_t = s_t, A_t = a_t]
  $$
- **含义**：表示在状态 $s_t$ 下采取动作 $a_t$，然后按照策略 $\pi$ 行动，能够获得的期望总奖励
- **特点**：依赖于状态和动作的组合

**优势函数 $A_t$ 的作用**：
- **相对评估**：优势函数衡量了某个动作相对于该状态下所有动作平均水平的优势
- **决策指导**：
  - 如果 $A_t > 0$，说明动作 $a_t$ 比平均水平好，应该增加其概率
  - 如果 $A_t < 0$，说明动作 $a_t$ 比平均水平差，应该减少其概率
  - 如果 $A_t = 0$，说明动作 $a_t$ 处于平均水平

**在PPO中的重要性**：
1. **减少方差**：相比直接使用奖励，优势函数提供了更稳定的学习信号
2. **基线作用**：状态价值函数作为基线，减少了策略梯度的方差
3. **相对比较**：通过相对比较而不是绝对奖励，使得训练更加稳定

**实际计算中的挑战**：
- 真实的价值函数通常是未知的，需要通过神经网络来估计
- 这就是为什么PPO需要Critic模型来估计状态价值函数
- 优势函数通常通过时序差分（TD）方法或其他技术来估计

### PPO 在 RLHF 中的应用

在RLHF中，PPO需要四个模型：

1. **Actor Model**：被训练的策略模型
2. **Critic Model**：价值函数模型，用于估计状态价值
3. **Reward Model**：奖励模型，用于计算即时奖励
4. **Reference Model**：参考模型，用于防止策略偏离太远

关于这 4 个模型，可以参考我之前的文章：[大模型RLHF训练中的PPO算法细节](https://murphypei.github.io/blog/2024/07/llm-rlhf-ppo.html)

**四个模型的作用和特点**：

**Actor Model（策略模型）**：
- 这是我们要训练的主要模型，最终用于实际应用
- 接收prompt，生成response
- 在训练过程中，其参数会不断更新以优化策略

**Critic Model（价值函数模型）**：
- 用于估计状态价值函数 $V(s_t)$
- 通常用Reward Model初始化，架构与Actor相似
- 在最后一层增加Value Head，输出单一的价值估计
- 需要更新参数，因为价值估计能力需要不断提升

**Reward Model（奖励模型）**：
- 计算即时奖励 $R_t$，评估当前response的好坏
- 参数固定不更新，作为客观的评估标准
- 只关心当前response的质量，不考虑长期影响

**Reference Model（参考模型）**：
- 通常用SFT模型初始化，参数冻结
- 主要作用是防止Actor"训歪"，避免过拟合到高分但无意义的回答
- 通过KL散度约束，确保新策略与参考策略的输出分布相似

**为什么需要四个模型**：
1. **Actor**：学习生成符合人类偏好的回答
2. **Critic**：评估整体价值，减少训练方差
3. **Reward**：提供客观的即时评估标准
4. **Reference**：防止策略偏离太远，保持语言能力

PPO 的损失函数包括三个部分：

$$
L_{PPO} = L^{CLIP} - \alpha L^{KL} + \beta L^{VF}
$$

其中：
- $L^{CLIP}$ 是 PPO 的主要损失，通过裁剪机制限制策略更新
- $L^{KL}$ 是 KL 散度损失，用于限制与参考模型的差异
- $L^{VF}$ 是价值函数损失，用于训练Critic模型
- $\alpha$ 和 $\beta$ 是权重参数，平衡不同损失项的重要性

**训练流程**：
1. Actor接收prompt，生成response
2. Reward Model计算即时奖励
3. Critic Model估计状态价值
4. 计算优势函数 $A_t = R_t - V_t$
5. 使用PPO损失函数更新Actor和Critic参数
6. 通过KL散度约束确保与Reference Model的相似性

## DPO（Direct Preference Optimization）

### DPO 基本原理

DPO 是一种更直接的方法，它不需要显式的奖励模型，而是直接通过人类偏好数据来优化策略。DPO 的核心思想是将偏好学习问题转化为一个分类问题。

**DPO的核心创新**：
1. **消除奖励模型**：不需要单独训练奖励模型，简化了训练流程
2. **直接偏好学习**：直接从人类偏好数据中学习，避免了奖励建模的误差
3. **理论等价性**：证明了DPO与基于奖励模型的RLHF在理论上是等价的

### DPO 的数学推导

#### 1. 偏好学习问题

给定一个提示 $x$ 和两个回答 $y_w$（获胜）和 $y_l$（失败），我们的目标是学习一个策略 $\pi_\theta$，使得：

$$
P(y_w \succ y_l | x) > P(y_l \succ y_w | x)
$$

**偏好数据的含义**：
- $y_w \succ y_l$ 表示在提示 $x$ 下，回答 $y_w$ 比 $y_l$ 更受人类偏好
- 这种偏好关系反映了人类的价值判断，是RLHF的核心数据

#### 2. Bradley-Terry 模型

DPO 使用 Bradley-Terry 模型来建模偏好。这个模型假设偏好概率与奖励函数之间存在以下关系：

$$
P(y_w \succ y_l | x) = \frac{\exp(r_\theta(x, y_w))}{\exp(r_\theta(x, y_w)) + \exp(r_\theta(x, y_l))}
$$

其中 $r_\theta(x, y)$ 是奖励函数。

**Bradley-Terry模型的特点**：
- **单调性**：奖励越高，被偏好的概率越大
- **对称性**：$P(y_w \succ y_l | x) + P(y_l \succ y_w | x) = 1$
- **温度控制**：可以通过调整指数函数的温度参数来控制偏好强度

#### 3. 从奖励函数到策略的映射

DPO的关键洞察是：我们可以将奖励函数表示为策略与参考策略的比值：

$$
r_\theta(x, y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}
$$

其中：
- $\pi_{ref}$ 是参考策略（通常是SFT模型）
- $\beta$ 是温度参数，控制奖励的强度

**这个映射的物理意义**：
- 如果 $\pi_\theta(y|x) > \pi_{ref}(y|x)$，说明新策略更倾向于生成回答 $y$，奖励为正
- 如果 $\pi_\theta(y|x) < \pi_{ref}(y|x)$，说明新策略不太倾向于生成回答 $y$，奖励为负
- $\beta$ 控制奖励的敏感度，值越大，策略差异对奖励的影响越明显

#### 4. DPO 的目标函数

将奖励函数代入Bradley-Terry模型，得到DPO的目标函数：

$$
L_{DPO} = -\mathbb{E}_{(x, y_w, y_l) \sim D} \left[\log \sigma\left(\beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\right)\right]
$$

其中：
- $\sigma$ 是 sigmoid 函数：$\sigma(x) = \frac{1}{1 + e^{-x}}$
- $\beta$ 是温度参数，通常设置为0.1-0.5
- $\pi_{ref}$ 是参考策略（通常是 SFT 模型）

**目标函数的直观理解**：
- 我们希望最大化偏好数据的对数似然
- 对于偏好对 $(y_w, y_l)$，我们希望 $P(y_w \succ y_l | x)$ 尽可能大
- 这等价于让 $\beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}$ 尽可能大
- 即让获胜回答相对于参考策略的提升幅度大于失败回答

#### 5. 奖励函数的推导

通过 DPO 的训练，我们可以推导出隐含的奖励函数：

$$
r_\theta(x, y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}
$$

这个公式表明，DPO 实际上是在学习一个相对于参考策略的奖励函数。

**奖励函数的性质**：
1. **相对性**：奖励是相对于参考策略定义的，不是绝对奖励
2. **可解释性**：奖励直接反映了策略相对于参考策略的偏好程度
3. **一致性**：与人类偏好数据保持一致

### DPO 的训练过程

**训练步骤**：
1. **数据准备**：收集人类偏好数据 $(x, y_w, y_l)$
2. **策略初始化**：用SFT模型初始化 $\pi_\theta$ 和 $\pi_{ref}$
3. **前向传播**：计算 $\pi_\theta(y_w|x)$ 和 $\pi_\theta(y_l|x)$
4. **损失计算**：使用DPO损失函数计算梯度
5. **参数更新**：只更新 $\pi_\theta$，保持 $\pi_{ref}$ 固定

**关键超参数**：
- **$\beta$（温度参数）**：控制策略更新的强度，值越大更新越激进
- **学习率**：控制参数更新的步长
- **批次大小**：影响训练的稳定性和效率

### DPO 的优势和局限性

**优势**：
1. **训练简单**：只需要两个模型，训练流程简单
2. **数据效率高**：直接使用偏好数据，避免了奖励建模的误差
3. **理论保证**：在理论上与基于奖励的RLHF等价
4. **计算效率高**：单轮训练，不需要复杂的优势估计

**局限性**：
1. **依赖参考策略**：奖励函数是相对于参考策略定义的
2. **偏好数据质量**：对偏好数据的质量要求较高
3. **探索能力有限**：可能无法探索到远离参考策略的新策略
4. **温度参数敏感**：$\beta$ 的选择对性能影响较大

### DPO 与 PPO 的理论联系

**等价性的核心前提**：

DPO与PPO等价的核心前提是：**奖励函数必须满足特定的形式**。具体来说，奖励函数必须能够表示为：

$$
r(x, y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}
$$

这个前提条件意味着：

1. **奖励函数与策略的耦合**：奖励函数不能是任意的，必须与当前策略 $\pi_\theta$ 和参考策略 $\pi_{ref}$ 相关
2. **相对性**：奖励是相对于参考策略定义的，不是绝对奖励
3. **策略依赖性**：奖励函数会随着策略的更新而变化

**为什么这个前提很重要**：

在实际的RLHF中，奖励函数通常是独立训练的，其形式为 $r(x, y) = f_\phi(x, y)$，其中 $f_\phi$ 是一个独立的神经网络。这种形式的奖励函数与DPO假设的形式完全不同。

**等价性的假设前提**：

DPO与PPO的等价性是在以下关键假设下成立的：

1. **奖励函数假设**：奖励函数必须满足 $r(x, y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}$ 的形式
2. **优势函数简化**：忽略价值函数，直接使用奖励作为优势函数，即 $A_t = r_t$
3. **策略约束方式**：使用KL散度约束而不是PPO的裁剪约束
4. **单步优化**：假设每次更新都是单步的，不考虑多步交互

**理论背景**：

这个等价性来自于**奖励函数与策略之间的对偶关系**。在强化学习中，存在一个重要的理论结果：对于任何奖励函数 $r(x, y)$，都存在一个最优策略 $\pi^*(y|x)$，使得：

$$
\pi^*(y|x) \propto \pi_{ref}(y|x) \exp(\frac{r(x, y)}{\beta})
$$

这个关系表明，奖励函数和策略之间存在一一对应的关系。DPO的关键洞察是：如果我们直接学习策略，就可以隐式地学习到对应的奖励函数。

**等价性证明的局限性**：

需要注意的是，这种等价性有以下局限性：

1. **奖励函数形式限制**：只有在特定形式的奖励函数下才成立
2. **忽略价值函数**：实际PPO中价值函数的作用被简化了
3. **约束方式不同**：PPO的裁剪约束和DPO的KL散度约束在理论上不等价
4. **训练稳定性**：虽然理论等价，但实际训练中的稳定性可能不同

**实际应用中的差异**：

尽管在理论上存在等价性，但在实际应用中：

1. **PPO**：通过显式奖励模型提供更直接的监督信号
2. **DPO**：通过偏好数据提供相对比较信号
3. **训练稳定性**：PPO的裁剪机制可能提供更好的训练稳定性
4. **探索能力**：PPO可能具有更好的探索能力

**等价性证明**：
DPO可以看作是PPO在特定条件下的简化版本。当PPO中的：
- 奖励模型 $r(x, y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}$
- 优势函数 $A_t = r_t$（忽略价值函数）
- 策略约束通过KL散度实现

此时，PPO的损失函数就退化为DPO的形式。

**主要区别**：
1. **奖励建模**：PPO需要显式奖励模型，DPO隐含在策略中
2. **优势估计**：PPO需要复杂的优势估计，DPO直接使用奖励
3. **约束方式**：PPO使用裁剪约束，DPO使用KL散度约束

## PPO 和 DPO 的区别

### 1. 训练复杂度

- **PPO**：需要四个模型（Actor、Critic、Reward、Reference），训练过程复杂
- **DPO**：只需要两个模型（策略模型和参考模型），训练过程简单

### 2. 数据需求

- **PPO**：需要显式的奖励信号或奖励模型
- **DPO**：只需要偏好数据（哪个更好），不需要显式奖励

### 3. 计算效率

- **PPO**：需要多轮交互和复杂的优势估计
- **DPO**：单轮训练，计算效率更高

### 4. 稳定性

- **PPO**：通过裁剪机制保证训练稳定性
- **DPO**：通过 KL 散度约束保证稳定性

### 5. 适用场景

- **PPO**：适用于有明确奖励信号或可以训练奖励模型的场景
- **DPO**：适用于只有人类偏好数据的场景

## 总结

PPO 和 DPO 都是 RLHF 中的重要算法，它们各有优缺点：

- **PPO** 更加成熟和稳定，但训练复杂度高
- **DPO** 更加简单和高效，但可能在某些场景下效果不如 PPO

选择哪种算法主要取决于具体的应用场景和可用的数据。在实际应用中，可以根据需求选择合适的算法，或者将两种算法结合使用。

## 参考文献

1. Schulman, J., et al. "Proximal policy optimization algorithms." arXiv preprint arXiv:1707.06347 (2017).
2. Rafailov, R., et al. "Direct preference optimization: Your language model is secretly a reward model." arXiv preprint arXiv:2305.18290 (2023).
